{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPjDtJVrAuzZOrSCnE6Izxb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThodupunooriSaiManish/Deep_Learning/blob/main/DL_Lab(Semester_6).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Week_1(19/01/26)"
      ],
      "metadata": {
        "id": "RAGqM1T89cMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1 Build a Simple Neural Network with PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "x = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
        "\n",
        "model = nn.Linear(1, 1)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(500):\n",
        "    pred = model(x)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "test = torch.tensor([[5.0]])\n",
        "print(\"Prediction:\", model(test).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_90YUDwW9Wjz",
        "outputId": "9dfdbaf8-55ba-43d6-d3c7-b3b6c631095b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 9.755949020385742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Simple Neural Network with TensorFlow\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3, 4], dtype=float)\n",
        "y = np.array([2, 4, 6, 8], dtype=float)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(1,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='sgd', loss='mse')\n",
        "\n",
        "model.fit(x, y, epochs=500, verbose=0)\n",
        "\n",
        "test = np.array([5.0])\n",
        "prediction = model.predict(test, verbose=0)\n",
        "\n",
        "print(\"Prediction:\", prediction[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC06OkKX9nki",
        "outputId": "fe960274-d858-488d-a785-7a6a08683eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 9.95343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Simple Neural Network with Using Keras (via tf.keras)\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3, 4], dtype=float)\n",
        "y = np.array([2, 4, 6, 8], dtype=float)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(1,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='sgd',\n",
        "    loss='mean_squared_error'\n",
        ")\n",
        "\n",
        "model.fit(x, y, epochs=300, verbose=0)\n",
        "\n",
        "test_data = np.array([5.0])\n",
        "prediction = model.predict(test_data, verbose=0)\n",
        "\n",
        "print(\"Prediction:\", prediction[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s716sTX1ACIU",
        "outputId": "1bd5df2c-346b-4146-fa89-377364f88a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 9.861242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 IMPLEMENT A SIMPLE PERCEPTRON (Coding a Neuron) and explain the meaning of feed-forward, step, and sigmoid functions\n",
        "import math\n",
        "\n",
        "def step_function(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "def perceptron(inputs, weights, bias):\n",
        "    weighted_sum = 0\n",
        "    for i in range(len(inputs)):\n",
        "        weighted_sum += inputs[i] * weights[i]\n",
        "\n",
        "    weighted_sum += bias\n",
        "    output = step_function(weighted_sum)\n",
        "    return output\n",
        "\n",
        "inputs = [1, 0]\n",
        "weights = [0.7, -0.4]\n",
        "bias = -0.2\n",
        "\n",
        "print(\"Perceptron Output:\", perceptron(inputs, weights, bias))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN7i5b0vAY-T",
        "outputId": "77f26827-9dd8-4ccb-e774-b8ba52912ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron Output: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Week_2(2/2/26)"
      ],
      "metadata": {
        "id": "MaJ65RcdA-Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1 Implement AND and OR logic operations using a single perceptron, and verify the correctness of the output using appropriate truth tables. (linear Data)\n",
        "def step_function(net):\n",
        "    if net >= 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def perceptron(x1, x2, w1, w2, b):\n",
        "    net = x1 * w1 + x2 * w2 + b\n",
        "    y = step_function(net)\n",
        "    return net, y\n",
        "\n",
        "print(\"AND Gate (with Step Function Output)\")\n",
        "print(\"x1 x2|Net Input|Step Output\")\n",
        "\n",
        "for x1 in [0, 1]:\n",
        "    for x2 in [0, 1]:\n",
        "        net, y = perceptron(x1, x2, 1, 1, -1.5)\n",
        "        print(x1, x2, \"|\", net, \"|\", y)\n",
        "\n",
        "print(\"\\nOR Gate (with Step Function Output)\")\n",
        "print(\"x1 x2|Net Input|Step Output\")\n",
        "\n",
        "for x1 in [0, 1]:\n",
        "    for x2 in [0, 1]:\n",
        "        net, y = perceptron(x1, x2, 1, 1, -0.5)\n",
        "        print(x1, x2, \"|\", net, \"|\", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ReTNrYvAp5a",
        "outputId": "91a0f448-4863-45f6-b1bc-f3645bef67bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate (with Step Function Output)\n",
            "x1 x2|Net Input|Step Output\n",
            "0 0 | -1.5 | 0\n",
            "0 1 | -0.5 | 0\n",
            "1 0 | -0.5 | 0\n",
            "1 1 | 0.5 | 1\n",
            "\n",
            "OR Gate (with Step Function Output)\n",
            "x1 x2|Net Input|Step Output\n",
            "0 0 | -0.5 | 0\n",
            "0 1 | 0.5 | 1\n",
            "1 0 | 0.5 | 1\n",
            "1 1 | 1.5 | 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Examine the feasibility of implementing the XOR and XNOR (¬XOR) operations (Non linear data) using a single perceptron.\n",
        "# If not possible, clearly explain the reason based on the concept of linear separability.\n",
        "def step(net):\n",
        "    return 1 if net >= 0 else 0\n",
        "\n",
        "def perceptron(x1, x2, w1, w2, b):\n",
        "    net = x1*w1 + x2*w2 + b\n",
        "    return step(net)\n",
        "\n",
        "def AND(x1, x2):\n",
        "    return perceptron(x1, x2, 1, 1, -1.5)\n",
        "\n",
        "def OR(x1, x2):\n",
        "    return perceptron(x1, x2, 1, 1, -0.5)\n",
        "\n",
        "def NOT(x):\n",
        "    return step(-1*x + 0.5)\n",
        "\n",
        "#XOR: (A OR B) AND NOT(A AND B)\n",
        "def XOR_attempt(x1, x2):\n",
        "    or_out = OR(x1, x2)\n",
        "    and_out = AND(x1, x2)\n",
        "    not_and = NOT(and_out)\n",
        "    return AND(or_out, not_and)\n",
        "\n",
        "#XNOR: NOT(XOR)\n",
        "def XNOR_attempt(x1, x2):\n",
        "    return NOT(XOR_attempt(x1, x2))\n",
        "\n",
        "print(\"XOR using AND, OR, NOT (Single Perceptron)\")\n",
        "print(\"x1 x2|Input|Output\")\n",
        "\n",
        "for x1 in [0, 1]:\n",
        "    for x2 in [0, 1]:\n",
        "        print(x1, x2, \"|\", net,\"|\", XOR_attempt(x1, x2))\n",
        "\n",
        "print(\"\\nXNOR using AND, OR, NOT (Single Perceptron)\")\n",
        "print(\"x1 x2|Input|Output\")\n",
        "\n",
        "for x1 in [0, 1]:\n",
        "    for x2 in [0, 1]:\n",
        "        print(x1, x2, \"|\", net, \"|\", XNOR_attempt(x1, x2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWM5eZT1HYWM",
        "outputId": "2135dfb5-8503-4f77-d6c8-d3c9ca56a795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR using AND, OR, NOT (Single Perceptron)\n",
            "x1 x2|Input|Output\n",
            "0 0 | 1.5 | 0\n",
            "0 1 | 1.5 | 1\n",
            "1 0 | 1.5 | 1\n",
            "1 1 | 1.5 | 0\n",
            "\n",
            "XNOR using AND, OR, NOT (Single Perceptron)\n",
            "x1 x2|Input|Output\n",
            "0 0 | 1.5 | 1\n",
            "0 1 | 1.5 | 0\n",
            "1 0 | 1.5 | 0\n",
            "1 1 | 1.5 | 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XOR can be expressed using AND, OR, and NOT operations, implementing it using only single perceptrons fails because XOR is not linearly separable. A single perceptron can only form linear decision boundaries. Hence, XOR and XNOR require a multi-layer perceptron with at least one hidden layer."
      ],
      "metadata": {
        "id": "D1Ee6FDRKEu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Implement the XOR and (¬XOR) logic operation using a multi-perceptron network,\n",
        "# and analyze how multiple perceptrons overcome the limitations of a single perceptron.\n",
        "def step(net):\n",
        "    return 1 if net >= 0 else 0\n",
        "\n",
        "def perceptron(x1, x2, w1, w2, b):\n",
        "    net = x1*w1 + x2*w2 + b\n",
        "    return step(net)\n",
        "\n",
        "def AND(x1, x2):\n",
        "    return perceptron(x1, x2, 1, 1, -1.5)\n",
        "\n",
        "def OR(x1, x2):\n",
        "    return perceptron(x1, x2, 1, 1, -0.5)\n",
        "\n",
        "def NOT(x):\n",
        "    return step(-1*x + 0.5)\n",
        "\n",
        "def XOR(x1, x2):\n",
        "    h1 = OR(x1, x2)\n",
        "    h2 = AND(x1, x2)\n",
        "    h2_not = NOT(h2)\n",
        "    return AND(h1, h2_not)\n",
        "\n",
        "def XNOR(x1, x2):\n",
        "    return NOT(XOR(x1, x2))\n",
        "\n",
        "print(\"XOR using Multi-Perceptron Network\")\n",
        "print(\"x1 x2|Input |Output\")\n",
        "\n",
        "for x1 in [0, 1]:\n",
        "    for x2 in [0, 1]:\n",
        "        print(x1, x2, \"|\", net, \"|\", XOR(x1, x2))\n",
        "\n",
        "print(\"\\nXNOR using Multi-Perceptron Network\")\n",
        "print(\"x1 x2|Input |Output\")\n",
        "\n",
        "for x1 in [0, 1]:\n",
        "    for x2 in [0, 1]:\n",
        "        print(x1, x2, \"|\", net, \"|\", XNOR(x1, x2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os9U4BaeJIc4",
        "outputId": "b367c11d-fe4b-4acf-d0db-2d6361ece42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR using Multi-Perceptron Network\n",
            "x1 x2|Input |Output\n",
            "0 0 | 1.5 | 0\n",
            "0 1 | 1.5 | 1\n",
            "1 0 | 1.5 | 1\n",
            "1 1 | 1.5 | 0\n",
            "\n",
            "XNOR using Multi-Perceptron Network\n",
            "x1 x2|Input |Output\n",
            "0 0 | 1.5 | 1\n",
            "0 1 | 1.5 | 0\n",
            "1 0 | 1.5 | 0\n",
            "1 1 | 1.5 | 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4 Demonstrate that the thresholding logic used by perceptron is very harsh.\n",
        "def step(net):\n",
        "    return 1 if net >= 0 else 0\n",
        "\n",
        "def perceptron(x1, x2, w1, w2, b):\n",
        "    net = x1*w1 + x2*w2 + b\n",
        "    return net, step(net)\n",
        "\n",
        "\n",
        "w1, w2, b = 1, 1, -1.5\n",
        "print(\"Demonstrating Harsh Thresholding (AND Gate)\")\n",
        "print(\"x1 x2 | Net Input | Output\")\n",
        "\n",
        "inputs = [(1, 1), (0.99, 1), (1, 0.99), (0.9, 1)]\n",
        "for x1, x2 in inputs:\n",
        "    net, y = perceptron(x1, x2, w1, w2, b)\n",
        "    print(round(x1,2), round(x2,2), \" | \", round(net,2), \" | \", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMU0Hlj-M3do",
        "outputId": "5203d665-dddc-4fda-daf1-d45a12db023a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Demonstrating Harsh Thresholding (AND Gate)\n",
            "x1 x2 | Net Input | Output\n",
            "1 1  |  0.5  |  1\n",
            "0.99 1  |  0.49  |  1\n",
            "1 0.99  |  0.49  |  1\n",
            "0.9 1  |  0.4  |  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5 Implement the Perceptron Learning Algorithm and\n",
        "# study the effect of weight updates on convergence for a binary decision problem such as determining whether a user would like to watch a movie.\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"f1\":[1,0,1,0,0],     # Matt Damon\n",
        "    \"f2\":[1,1,0,0,1],     # Thriller\n",
        "    \"f3\":[0,1,1,0,0],     # Nolan\n",
        "    \"f4\":[0.9,0.7,0.6,0.3,0.4],  # IMDb\n",
        "    \"y\":[1,1,1,0,0]       # Like\n",
        "})\n",
        "\n",
        "df.to_csv(\"movies.csv\", index=False)"
      ],
      "metadata": {
        "id": "aryZTbaBOQnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data = pd.read_csv(\"movies.csv\")\n",
        "X = data.iloc[:,:4].values\n",
        "y = data.iloc[:,4].values\n",
        "\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0"
      ],
      "metadata": {
        "id": "u137BmGCk8AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MP Perceptron\")\n",
        "for i in range(len(X)):\n",
        "    print(step(sum(X[i])-2), \"Actual:\", y[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvXWHvfnlA86",
        "outputId": "776b8f1c-2570-4694-8faa-3e6a2f85453e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MP Perceptron\n",
            "1 Actual: 1\n",
            "1 Actual: 1\n",
            "1 Actual: 1\n",
            "0 Actual: 0\n",
            "0 Actual: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.zeros(4)\n",
        "lr = 0.1\n",
        "\n",
        "for i in range(len(X)):\n",
        "    y_pred = step(np.dot(w, X[i]))\n",
        "    w += lr * (y[i]-y_pred) * X[i]\n",
        "\n",
        "print(\"Weights only:\", w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QluAM_H5lDyU",
        "outputId": "b266d683-f45a-45a9-fb0a-97331b858c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights only: [ 0.    0.    0.   -0.03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.zeros(4)\n",
        "b = 0\n",
        "\n",
        "for i in range(len(X)):\n",
        "    y_pred = step(np.dot(w, X[i]) + b)\n",
        "    w += lr * (y[i]-y_pred) * X[i]\n",
        "    b += lr * (y[i]-y_pred)\n",
        "\n",
        "print(\"Weights:\", w, \"Bias:\", b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj1iWj3plLnD",
        "outputId": "89040f73-b6a6-4765-e9ed-7a97c10b7655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [ 0.    0.    0.   -0.03] Bias: -0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = np.array([1,0,0,0.8])\n",
        "print(\"Prediction:\", step(np.dot(w, sample) + b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hykIV2NlbYq",
        "outputId": "6f570bba-a849-40ad-957e-1225c2b467e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MP Perceptron fails because it does not update weights.\n",
        "Perceptron with weights learns feature importance but lacks flexibility without bias.\n",
        "Adding bias improves convergence and classification accuracy.\n",
        "Weight and bias updates are essential for effective learning in perceptron models."
      ],
      "metadata": {
        "id": "_TWsfd0l07JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Demonstrate the Representation Power of a Network of Perceptrons\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "\n",
        "# Step function\n",
        "def step(net):\n",
        "    return 1 if net >= 0 else 0\n",
        "\n",
        "# Perceptron training function\n",
        "def train_perceptron(y, epochs=20, lr=0.1):\n",
        "    w = np.zeros(2)\n",
        "    b = 0\n",
        "    for _ in range(epochs):\n",
        "        for i in range(4):\n",
        "            pred = step(np.dot(w, X[i]) + b)\n",
        "            w += lr * (y[i] - pred) * X[i]\n",
        "            b += lr * (y[i] - pred)\n",
        "    return w, b\n",
        "\n",
        "# Generating all 16 Boolean functions\n",
        "boolean_functions = []\n",
        "for i in range(16):\n",
        "    boolean_functions.append(\n",
        "        [(i >> j) & 1 for j in range(4)][::-1]\n",
        "    )\n",
        "\n",
        "not_learnable = []\n",
        "\n",
        "# Testing each Boolean function\n",
        "for idx, y in enumerate(boolean_functions):\n",
        "    w, b = train_perceptron(y)\n",
        "    preds = [step(np.dot(w, X[i]) + b) for i in range(4)]\n",
        "    if preds != y:\n",
        "        not_learnable.append(idx+1)\n",
        "\n",
        "print(\"Total Boolean functions:\", 16)\n",
        "print(\"Not learnable by single perceptron:\", not_learnable)\n",
        "print(\"Count:\", len(not_learnable))"
      ],
      "metadata": {
        "id": "hdLbwL9Bl8Pw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2363a73d-9c54-4441-b2cc-cefc1d5a220b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Boolean functions: 16\n",
            "Not learnable by single perceptron: [7, 10]\n",
            "Count: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "for n in [2, 3, 4]:\n",
        "    total = 2 ** (2 ** n)\n",
        "    print(f\"Inputs = {n}, Total Boolean functions = {total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QUmToUP3LvG",
        "outputId": "dca6a400-5db7-4429-c033-055a53266506"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs = 2, Total Boolean functions = 16\n",
            "Inputs = 3, Total Boolean functions = 256\n",
            "Inputs = 4, Total Boolean functions = 65536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) 2^2^n\n"
      ],
      "metadata": {
        "id": "_ejzNxsc2erN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A single perceptron can represent only linearly separable Boolean functions. For two inputs, out of 16 Boolean functions, only 14 are linearly separable, while XOR and XNOR are not. As the number of inputs increases, the number of non-linearly separable Boolean functions grows exponentially, demonstrating the limited representation power of a single perceptron and the need for multi-layer networks."
      ],
      "metadata": {
        "id": "OgS5lN042U4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n"
      ],
      "metadata": {
        "id": "mYSNp6je2WRX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}